{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1."
      ],
      "metadata": {
        "id": "KuleSCPmQg_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples = 1000\n",
        "\n",
        "X1 = np.random.rand(n_samples)\n",
        "X2 = X1 + np.random.normal(0, 0.01, n_samples)\n",
        "X3 = X1 + np.random.normal(0, 0.01, n_samples)\n",
        "X4 = X2 + np.random.normal(0, 0.01, n_samples)\n",
        "X5 = X3 + np.random.normal(0, 0.01, n_samples)\n",
        "X6 = X5 + np.random.normal(0, 0.01, n_samples)\n",
        "X7 = X4 + np.random.normal(0, 0.01, n_samples)\n",
        "\n",
        "X = np.stack([X1, X2, X3, X4, X5, X6, X7], axis=1)\n",
        "y = 3 * X1 + 2 * X2 + 0.5 * X7 + np.random.normal(0, 0.1, n_samples)\n",
        "\n",
        "df = pd.DataFrame(X, columns=[f'X{i}' for i in range(1, 8)])\n",
        "df['y'] = y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Dzwb5kFxPsBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"Number of NaN values in X_train:\", np.isnan(X_train_scaled).sum())\n",
        "print(\"Number of NaN values in X_test:\", np.isnan(X_test_scaled).sum())\n",
        "print(\"Number of NaN values in y_train:\", np.isnan(y_train).sum())\n",
        "print(\"Number of NaN values in y_test:\", np.isnan(y_test).sum())\n",
        "if (np.isnan(X_train_scaled).sum() > 0 or np.isnan(y_train).sum() > 0):\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "\n",
        "    X_train_scaled = imputer.fit_transform(X_train_scaled)\n",
        "    X_test_scaled = imputer.transform(X_test_scaled)\n",
        "    y_train = imputer.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "    y_test = imputer.transform(y_test.reshape(-1, 1)).ravel()\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "regularization_params = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "best_r2 = -float(\"inf\")\n",
        "best_lr = None\n",
        "best_reg = None\n",
        "best_cost = None\n",
        "class RidgeRegressionGD:\n",
        "    def __init__(self, learning_rate=0.01, reg_param=1, n_iters=500):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.reg_param = reg_param\n",
        "        self.n_iters = n_iters\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros(n)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            dw = (1/m) * np.dot(X.T, (y_pred - y)) + 2 * self.reg_param * self.weights\n",
        "            db = (1/m) * np.sum(y_pred - y)\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def compute_cost(self, X, y):\n",
        "        m = len(y)\n",
        "        y_pred = np.dot(X, self.weights) + self.bias\n",
        "        cost = (1 / (2 * m)) * np.sum((y_pred - y) ** 2) + self.reg_param * np.sum(self.weights ** 2)\n",
        "        return cost\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for reg in regularization_params:\n",
        "        model = RidgeRegressionGD(learning_rate=lr, reg_param=reg, n_iters=1000)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        predictions = model.predict(X_test_scaled)\n",
        "        if np.isnan(predictions).any():\n",
        "            continue\n",
        "\n",
        "        r2 = r2_score(y_test, predictions)\n",
        "        cost = model.compute_cost(X_test_scaled, y_test)\n",
        "\n",
        "        print(f\"Learning Rate: {lr}, Regularization: {reg}, R2_score: {r2:.4f}, Cost: {cost:.4f}\")\n",
        "\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_lr = lr\n",
        "            best_reg = reg\n",
        "            best_cost = cost\n",
        "best_lr, best_reg, best_r2, best_cost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsxYmwZCPtRi",
        "outputId": "8872eee0-de22-40f9-cd1d-36b909665b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN values in X_train: 0\n",
            "Number of NaN values in X_test: 0\n",
            "Number of NaN values in y_train: 0\n",
            "Number of NaN values in y_test: 0\n",
            "Learning Rate: 0.0001, Regularization: 1e-15, R2_score: -1.5199, Cost: 3.1713\n",
            "Learning Rate: 0.0001, Regularization: 1e-10, R2_score: -1.5199, Cost: 3.1713\n",
            "Learning Rate: 0.0001, Regularization: 1e-05, R2_score: -1.5199, Cost: 3.1713\n",
            "Learning Rate: 0.0001, Regularization: 0.001, R2_score: -1.5199, Cost: 3.1714\n",
            "Learning Rate: 0.0001, Regularization: 0, R2_score: -1.5199, Cost: 3.1713\n",
            "Learning Rate: 0.0001, Regularization: 1, R2_score: -1.5518, Cost: 3.2901\n",
            "Learning Rate: 0.0001, Regularization: 10, R2_score: -1.7776, Cost: 3.7115\n",
            "Learning Rate: 0.0001, Regularization: 20, R2_score: -1.9042, Cost: 3.8157\n",
            "Learning Rate: 0.001, Regularization: 1e-15, R2_score: 0.6017, Cost: 0.5013\n",
            "Learning Rate: 0.001, Regularization: 1e-10, R2_score: 0.6017, Cost: 0.5013\n",
            "Learning Rate: 0.001, Regularization: 1e-05, R2_score: 0.6017, Cost: 0.5013\n",
            "Learning Rate: 0.001, Regularization: 0.001, R2_score: 0.6017, Cost: 0.5016\n",
            "Learning Rate: 0.001, Regularization: 0, R2_score: 0.6017, Cost: 0.5013\n",
            "Learning Rate: 0.001, Regularization: 1, R2_score: 0.5777, Cost: 0.7550\n",
            "Learning Rate: 0.001, Regularization: 10, R2_score: 0.1343, Cost: 1.3375\n",
            "Learning Rate: 0.001, Regularization: 20, R2_score: -0.0299, Cost: 1.4599\n",
            "Learning Rate: 0.01, Regularization: 1e-15, R2_score: 0.9958, Cost: 0.0053\n",
            "Learning Rate: 0.01, Regularization: 1e-10, R2_score: 0.9958, Cost: 0.0053\n",
            "Learning Rate: 0.01, Regularization: 1e-05, R2_score: 0.9958, Cost: 0.0053\n",
            "Learning Rate: 0.01, Regularization: 0.001, R2_score: 0.9958, Cost: 0.0056\n",
            "Learning Rate: 0.01, Regularization: 0, R2_score: 0.9958, Cost: 0.0053\n",
            "Learning Rate: 0.01, Regularization: 1, R2_score: 0.9470, Cost: 0.2903\n",
            "Learning Rate: 0.01, Regularization: 10, R2_score: 0.4452, Cost: 0.9462\n",
            "Learning Rate: 0.01, Regularization: 20, R2_score: 0.2686, Cost: 1.0841\n",
            "Learning Rate: 0.1, Regularization: 1e-15, R2_score: 0.9959, Cost: 0.0052\n",
            "Learning Rate: 0.1, Regularization: 1e-10, R2_score: 0.9959, Cost: 0.0052\n",
            "Learning Rate: 0.1, Regularization: 1e-05, R2_score: 0.9959, Cost: 0.0052\n",
            "Learning Rate: 0.1, Regularization: 0.001, R2_score: 0.9959, Cost: 0.0056\n",
            "Learning Rate: 0.1, Regularization: 0, R2_score: 0.9959, Cost: 0.0052\n",
            "Learning Rate: 0.1, Regularization: 1, R2_score: 0.9470, Cost: 0.2903\n",
            "Learning Rate: 0.1, Regularization: 10, R2_score: -inf, Cost: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1220: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "<ipython-input-9-d5a8b3cd173b>:58: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum((y_pred - y) ** 2) + self.reg_param * np.sum(self.weights ** 2)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "<ipython-input-9-d5a8b3cd173b>:47: RuntimeWarning: invalid value encountered in multiply\n",
            "  dw = (1/m) * np.dot(X.T, (y_pred - y)) + 2 * self.reg_param * self.weights\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1, 1e-15, 0.995876532335217, 0.005189314144506638)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2."
      ],
      "metadata": {
        "id": "2DZwAdGFQoGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "hitters_df = pd.read_csv(\"Hitters.csv\")\n",
        "print(hitters_df.isnull().sum())\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "hitters_df['Salary'] = num_imputer.fit_transform(hitters_df[['Salary']])\n",
        "hitters_df.dropna(inplace=True)\n",
        "categorical_columns = hitters_df.select_dtypes(include=['object']).columns\n",
        "hitters_df = pd.get_dummies(hitters_df, columns=categorical_columns, drop_first=True)\n",
        "print(hitters_df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTlT1eJERNKi",
        "outputId": "4b8b4e03-bc50-441b-f94d-41d128e24450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AtBat         0\n",
            "Hits          0\n",
            "HmRun         0\n",
            "Runs          0\n",
            "RBI           0\n",
            "Walks         0\n",
            "Years         0\n",
            "CAtBat        0\n",
            "CHits         0\n",
            "CHmRun        0\n",
            "CRuns         0\n",
            "CRBI          0\n",
            "CWalks        0\n",
            "League        0\n",
            "Division      0\n",
            "PutOuts       0\n",
            "Assists       0\n",
            "Errors        0\n",
            "Salary       59\n",
            "NewLeague     0\n",
            "dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 322 entries, 0 to 321\n",
            "Data columns (total 20 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   AtBat        322 non-null    int64  \n",
            " 1   Hits         322 non-null    int64  \n",
            " 2   HmRun        322 non-null    int64  \n",
            " 3   Runs         322 non-null    int64  \n",
            " 4   RBI          322 non-null    int64  \n",
            " 5   Walks        322 non-null    int64  \n",
            " 6   Years        322 non-null    int64  \n",
            " 7   CAtBat       322 non-null    int64  \n",
            " 8   CHits        322 non-null    int64  \n",
            " 9   CHmRun       322 non-null    int64  \n",
            " 10  CRuns        322 non-null    int64  \n",
            " 11  CRBI         322 non-null    int64  \n",
            " 12  CWalks       322 non-null    int64  \n",
            " 13  PutOuts      322 non-null    int64  \n",
            " 14  Assists      322 non-null    int64  \n",
            " 15  Errors       322 non-null    int64  \n",
            " 16  Salary       322 non-null    float64\n",
            " 17  League_N     322 non-null    bool   \n",
            " 18  Division_W   322 non-null    bool   \n",
            " 19  NewLeague_N  322 non-null    bool   \n",
            "dtypes: bool(3), float64(1), int64(16)\n",
            "memory usage: 43.8 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X = hitters_df.drop(columns=['Salary'])\n",
        "y = hitters_df['Salary']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(f\"X_train shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test shape: {X_test_scaled.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS6Q-LsKUJ0V",
        "outputId": "74960ae2-d340-4f52-a92b-48ca11479f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (257, 19)\n",
            "X_test shape: (65, 19)\n",
            "y_train shape: (257,)\n",
            "y_test shape: (65,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "ridge_model = Ridge(alpha=0.5748)\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "lasso_model = Lasso(alpha=0.5748)\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "print(\"Linear Regression Coefficients:\", linear_model.coef_)\n",
        "print(\"Ridge Regression Coefficients:\", ridge_model.coef_)\n",
        "print(\"LASSO Regression Coefficients:\", lasso_model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2bRyjJBU5w7",
        "outputId": "042b3781-cb31-46d7-c9a2-49e9efa0ba45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Coefficients: [-280.34211773  254.85642954   51.06688158  -23.32191325  -51.72982757\n",
            "  109.44151762  -39.64710488 -560.66509814  407.02049119  -11.01674092\n",
            "  297.20460427  286.33983861 -205.9936606    51.68994142   44.23399217\n",
            "  -20.90083424   18.6843613   -50.6208016   -12.24371842]\n",
            "Ridge Regression Coefficients: [-290.55944588  258.90121297   39.83326588  -20.25178104  -37.74330697\n",
            "  109.98327779  -60.76017277 -325.36790562  255.74410895    6.28951331\n",
            "  267.6023989   241.2276548  -214.05273468   52.30163036   39.60185055\n",
            "  -20.03254578   16.05457488  -52.30078515  -10.87485569]\n",
            "LASSO Regression Coefficients: [-283.34161179  242.81113277   32.70143979   -8.92295822  -28.44998553\n",
            "  103.98988805  -52.21848263 -383.07585691  291.26643367    0.\n",
            "  266.86780871  252.83100034 -203.67909711   51.83703983   38.53611158\n",
            "  -18.40175814   11.58836304  -51.06696766   -5.84414368]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.032e+03, tolerance: 4.096e+03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "linear_predictions = linear_model.predict(X_test_scaled)\n",
        "ridge_predictions = ridge_model.predict(X_test_scaled)\n",
        "lasso_predictions = lasso_model.predict(X_test_scaled)\n",
        "linear_r2 = r2_score(y_test, linear_predictions)\n",
        "linear_mse = mean_squared_error(y_test, linear_predictions)\n",
        "ridge_r2 = r2_score(y_test, ridge_predictions)\n",
        "ridge_mse = mean_squared_error(y_test, ridge_predictions)\n",
        "lasso_r2 = r2_score(y_test, lasso_predictions)\n",
        "lasso_mse = mean_squared_error(y_test, lasso_predictions)\n",
        "print(f\"Linear Regression - R^2: {linear_r2:.4f}, MSE: {linear_mse:.4f}\")\n",
        "print(f\"Ridge Regression - R^2: {ridge_r2:.4f}, MSE: {ridge_mse:.4f}\")\n",
        "print(f\"LASSO Regression - R^2: {lasso_r2:.4f}, MSE: {lasso_mse:.4f}\")\n",
        "if linear_r2 > ridge_r2 and linear_r2 > lasso_r2:\n",
        "    print(\"Linear Regression performs the best.\")\n",
        "elif ridge_r2 > linear_r2 and ridge_r2 > lasso_r2:\n",
        "    print(\"Ridge Regression performs the best.\")\n",
        "else:\n",
        "    print(\"LASSO Regression performs the best.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F7QoTF8VqvD",
        "outputId": "5e5e42dc-838f-4af0-a7f0-1ca1f50e2a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression - R^2: 0.3369, MSE: 124847.6947\n",
            "Ridge Regression - R^2: 0.3521, MSE: 121982.3621\n",
            "LASSO Regression - R^2: 0.3519, MSE: 122022.6426\n",
            "Ridge Regression performs the best.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3."
      ],
      "metadata": {
        "id": "5xnj_VXaVy_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "alphas = np.logspace(-6, 6, 13)\n",
        "ridge_cv = RidgeCV(alphas=alphas, scoring='r2', cv=5)\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_cv.predict(X_test)\n",
        "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "y_pred_lasso = lasso_cv.predict(X_test)\n",
        "ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
        "lasso_r2 = r2_score(y_test, y_pred_lasso)\n",
        "ridge_mse = mean_squared_error(y_test, y_pred_ridge)\n",
        "lasso_mse = mean_squared_error(y_test, y_pred_lasso)\n",
        "print(f\"RidgeCV - Best Alpha: {ridge_cv.alpha_}, R²: {ridge_r2:.4f}, MSE: {ridge_mse:.4f}\")\n",
        "print(f\"LassoCV - Best Alpha: {lasso_cv.alpha_}, R²: {lasso_r2:.4f}, MSE: {lasso_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVXtJiAOV02I",
        "outputId": "138794cd-66de-4ec9-974f-ab0580e0dc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RidgeCV - Best Alpha: 1e-06, R²: 0.5758, MSE: 0.5559\n",
            "LassoCV - Best Alpha: 0.001, R²: 0.5769, MSE: 0.5545\n"
          ]
        }
      ]
    }
  ]
}